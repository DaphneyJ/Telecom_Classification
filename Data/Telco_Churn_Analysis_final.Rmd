---
title: "Telco Customer Churn Analysis"
output: html_document
---

## Load required libraries

```{r}
library(dplyr)
library(ggplot2)
library(caret)
library(glmnet)
library(MASS)
library(corrplot)
library(pROC)
library(randomForest)
library(gbm)
library(xgboost)
library(cluster)
library(factoextra)
```

## Load and merge data

```{r}
status <- read.csv("Telco_customer_churn_status.csv")
demographics <- read.csv("Telco_customer_churn_demographics.csv")
location <- read.csv("Telco_customer_churn_location.csv")
services <- read.csv("Telco_customer_churn_services.csv")

data <- status %>%
  left_join(demographics, by = "Customer.ID") %>%
  left_join(location, by = "Customer.ID") %>%
  left_join(services, by = "Customer.ID")
```

## Select and clean variables

```{r}
selected_vars <- c("Churn.Label", "Gender", "Senior.Citizen", "Married", "Dependents", 
                   "Number.of.Dependents", "Tenure.in.Months", "Contract", 
                   "Phone.Service", "Multiple.Lines", "Internet.Service", 
                   "Online.Security", "Online.Backup", "Device.Protection.Plan", 
                   "Premium.Tech.Support", "Streaming.TV", "Streaming.Movies", 
                   "Paperless.Billing", "Payment.Method", "Monthly.Charge", 
                   "Total.Charges", "CLTV", "Satisfaction.Score")

data_model <- data %>%
  dplyr::select(all_of(selected_vars)) %>%
  na.omit() %>%
  mutate(Churn = as.factor(Churn.Label)) %>%
  dplyr::select(-Churn.Label)

```

## Exploratory Data Analysis

```{r}
churn_counts <- table(data_model$Churn)
churn_counts_df <- as.data.frame(churn_counts)
colnames(churn_counts_df) <- c("Churn", "count")
churn_counts_df$percentage <- (churn_counts_df$count / sum(churn_counts_df$count)) * 100
ggplot(churn_counts_df, aes(x = Churn, y = count)) + 
  geom_bar(stat = "identity", fill = "#FF6666") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), 
            vjust = -0.5, size = 5) + 
  labs(title = "Churn Distribution")

ggplot(data_model, aes(x = Churn, y = Monthly.Charge, fill = Churn)) +
  geom_boxplot() + labs(title = "Monthly Charge by Churn")

ggplot(data_model, aes(x = Contract, fill = Churn)) +
  geom_bar(position = "fill") + scale_y_continuous(labels = scales::percent) +
  labs(title = "Churn Rate by Contract Type")

num_vars <- data_model[, sapply(data_model, is.numeric)]
corrplot(cor(num_vars), method = "color", type = "upper", tl.cex = 0.8)
```

## Train/test split and preprocessing

```{r}
set.seed(42)
split <- createDataPartition(data_model$Churn, p = 0.8, list = FALSE)
train <- data_model[split, ]
test  <- data_model[-split, ]

train <- train %>% mutate(across(where(is.character), as.factor))
test  <- test %>% mutate(across(where(is.character), as.factor))

valid_factors <- sapply(train, function(x) {
  if (is.factor(x)) nlevels(x) > 1 else TRUE
})
train <- train[, valid_factors]
test <- test[, names(train)]

nzv <- nearZeroVar(train, saveMetrics = TRUE)
train <- train[, !nzv$zeroVar]
test <- test[, names(train)]
```

## Stepwise Logistic Regression

```{r}
full_model <- glm(Churn ~ ., data = train, family = "binomial")
step_model <- stepAIC(full_model, direction = "both", trace = FALSE)

step_pred_prob <- predict(step_model, newdata = test, type = "response")
step_pred <- ifelse(step_pred_prob > 0.5, "Yes", "No")

confusionMatrix(as.factor(step_pred), test$Churn)
step_roc <- roc(test$Churn, step_pred_prob)
plot(step_roc, main = "Stepwise Logistic ROC")
auc(step_roc)
```

## LASSO Logistic Regression

```{r}
x_train <- model.matrix(Churn ~ ., data = train)[, -1]
y_train <- ifelse(train$Churn == "Yes", 1, 0)
x_test <- model.matrix(Churn ~ ., data = test)[, -1]
y_test <- ifelse(test$Churn == "Yes", 1, 0)

cv_lasso <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1)
lasso_model <- glmnet(x_train, y_train, family = "binomial", lambda = cv_lasso$lambda.min)
lasso_pred_prob <- predict(lasso_model, newx = x_test, type = "response")
lasso_pred <- ifelse(lasso_pred_prob > 0.5, 1, 0)
confusionMatrix(as.factor(lasso_pred), as.factor(y_test))
lasso_roc <- roc(y_test, as.vector(lasso_pred_prob))
plot(lasso_roc, main = "LASSO Logistic ROC")
auc(lasso_roc)
```

## Model comparison table

```{r}
results <- data.frame(
  Model = c("Stepwise Logistic", "LASSO Logistic"),
  Accuracy = c(
    sum(step_pred == test$Churn) / length(step_pred),
    sum(lasso_pred == y_test) / length(lasso_pred)
  ),
  AUC = c(auc(step_roc), auc(lasso_roc))
)
print(results)
```


## Extended models: RF, GBM, XGBoost, Clustering

```{r}
rf_model <- randomForest(Churn ~ ., data = train, ntree = 100)
rf_pred <- predict(rf_model, newdata = test)
confusionMatrix(rf_pred, test$Churn)
varImpPlot(rf_model)

# Copy train and test sets with numeric target for gbm
train_gbm <- train %>%
  mutate(Churn = ifelse(Churn == "Yes", 1, 0))

test_gbm <- test %>%
  mutate(Churn = ifelse(Churn == "Yes", 1, 0))

# Train GBM
gbm_model <- gbm(Churn ~ ., data = train_gbm, distribution = "bernoulli",
                 n.trees = 300, shrinkage = 0.01, interaction.depth = 3, cv.folds = 5)

best_iter <- gbm.perf(gbm_model, method = "cv")

# Predict and evaluate
gbm_pred_prob <- predict(gbm_model, newdata = test_gbm, n.trees = best_iter, type = "response")
gbm_pred <- ifelse(gbm_pred_prob > 0.5, 1, 0)
confusionMatrix(as.factor(gbm_pred), as.factor(test_gbm$Churn))


xgb_train <- xgb.DMatrix(data = x_train, label = y_train)
xgb_test <- xgb.DMatrix(data = x_test, label = y_test)
xgb_model <- xgboost(data = xgb_train, objective = "binary:logistic", nrounds = 100, verbose = 0)
xgb_pred_prob <- predict(xgb_model, newdata = xgb_test)
xgb_pred <- ifelse(xgb_pred_prob > 0.5, 1, 0)
confusionMatrix(as.factor(xgb_pred), as.factor(y_test))

numeric_scaled <- scale(data_model[, sapply(data_model, is.numeric)])
fviz_nbclust(numeric_scaled, kmeans, method = "wss")
km <- kmeans(numeric_scaled, centers = 3)
fviz_cluster(km, data = numeric_scaled)
```

## Model Comparison

```{r}
# Model comparison for Stepwise, LASSO, RF, GBM, XGBoost

# Convert any needed factors for fairness
rf_auc <- roc(test$Churn, as.numeric(rf_pred == "Yes"))
gbm_auc <- roc(test_gbm$Churn, gbm_pred_prob)
xgb_auc <- roc(y_test, xgb_pred_prob)

results_all <- data.frame(
  Model = c("Stepwise Logistic", "LASSO Logistic", "Random Forest", "GBM", "XGBoost"),
  Accuracy = c(
    mean(step_pred == test$Churn),
    mean(lasso_pred == y_test),
    mean(rf_pred == test$Churn),
    mean(gbm_pred == test_gbm$Churn),
    mean(xgb_pred == y_test)
  ),
  AUC = c(
    auc(step_roc),
    auc(lasso_roc),
    auc(rf_auc),
    auc(gbm_auc),
    auc(xgb_auc)
  )
)

print(results_all)

```


# Cross-Validation

```{r}

set.seed(123)
B=100
monte_df<-data.frame()


for (b in 1:B){
  if (b %% 20 == 0) {
    print(paste("Loop", b, "of", B))
  }
  
  split <- createDataPartition(data_model$Churn, p = 0.8, list = FALSE)
  train <- data_model[split, ]
  test  <- data_model[-split, ]
  
  # Data Pre-processing
  train <- train %>% mutate(across(where(is.character), as.factor))
  test  <- test %>% mutate(across(where(is.character), as.factor))

  valid_factors <- sapply(train, function(x) {
    if (is.factor(x)) nlevels(x) > 1 else TRUE
  })
  train <- train[, valid_factors]
  test <- test[, names(train)]

  nzv <- nearZeroVar(train, saveMetrics = TRUE)
  train <- train[, !nzv$zeroVar]
  test <- test[, names(train)]

  
  #Prepare Data 
  x_train <- model.matrix(Churn ~ ., data = train)[, -1]
  y_train <- ifelse(train$Churn == "Yes", 1, 0)
  x_test <- model.matrix(Churn ~ ., data = test)[, -1]
  y_test <- ifelse(test$Churn == "Yes", 1, 0)


  ### Model 1: Stepwise
  full_model <- glm(Churn ~ ., data = train, family = "binomial")
  step_model <- stepAIC(full_model, direction = "both", trace = FALSE)
  step_pred_prob <- predict(step_model, newdata = test, type = "response")
  step_pred <- ifelse(step_pred_prob > 0.5, "Yes", "No")

  ### Model 2: LASSO
  lasso_model <- glmnet(x_train, y_train, family = "binomial", lambda = cv_lasso$lambda.min)
  lasso_pred_prob <- predict(lasso_model, newx = x_test, type = "response")
  lasso_pred <- ifelse(lasso_pred_prob > 0.5, 1, 0)

  ### Model 3: Random Forest
  rf_model <- randomForest(Churn ~ ., data = train, ntree = 100) 
  rf_pred_prob <- predict(rf_model, newdata = test, type = "prob")[, "Yes"] 
  rf_pred <- predict(rf_model, newdata = test) 
  
  ### Model 4: GBM
  train_gbm <- train %>%
    mutate(Churn = ifelse(Churn == "Yes", 1, 0))
  test_gbm <- test %>%
    mutate(Churn = ifelse(Churn == "Yes", 1, 0))

  gbm_model <- gbm(Churn ~ ., data = train_gbm, distribution = "bernoulli",
                 n.trees = best_iter, shrinkage = 0.01, interaction.depth = 3)

  gbm_pred_prob <- predict(gbm_model, newdata = test_gbm, n.trees = best_iter, type = "response")
  gbm_pred <- ifelse(gbm_pred_prob > 0.5, 1, 0)

  
  ### Model 5: XGBoost
  xgb_train <- xgb.DMatrix(data = x_train, label = y_train)
  xgb_test <- xgb.DMatrix(data = x_test, label = y_test)
  xgb_model <- xgboost(data = xgb_train, objective = "binary:logistic", nrounds = 100, verbose = 0)
  xgb_pred_prob <- predict(xgb_model, newdata = xgb_test)
  xgb_pred <- ifelse(xgb_pred_prob > 0.5, 1, 0)

  ### Metrics
  step_table<-confusionMatrix(as.factor(step_pred), test$Churn)
  lasso_table<-confusionMatrix(as.factor(lasso_pred), as.factor(y_test))
  rf_table<-confusionMatrix(as.factor(rf_pred), test$Churn)
  gbm_table<-confusionMatrix(as.factor(gbm_pred), as.factor(test_gbm$Churn))
  xg_table<-confusionMatrix(as.factor(xgb_pred), as.factor(y_test))

  lasso_roc <- roc(y_test, as.vector(lasso_pred_prob))
  step_roc <- roc(y_test, as.vector(step_pred_prob))
  rf_roc <- roc(y_test, as.vector(rf_pred_prob))  
  gbm_roc <- roc(y_test, as.vector(gbm_pred_prob))
  xgb_roc <- roc(y_test, as.vector(xgb_pred_prob))


  monte_df<-rbind(monte_df,c(
    lasso_roc$auc,
    step_roc$auc,
    rf_roc$auc,  
    gbm_roc$auc,
    xgb_roc$auc,
    
    lasso_table$byClass['F1'],
    step_table$byClass['F1'],
    gbm_table$byClass['F1'],
    rf_table$byClass['F1'],
    xg_table$byClass['F1'],
    
    lasso_table$byClass['Precision'],
    step_table$byClass['Precision'],
    gbm_table$byClass['Precision'],
    rf_table$byClass['Precision'],
    xg_table$byClass['Precision'],
    
    lasso_table$byClass['Recall'],
    step_table$byClass['Recall'],
    gbm_table$byClass['Recall'],
    rf_table$byClass['Recall'],
    xg_table$byClass['Recall'],
    
    lasso_table$byClass['Balanced Accuracy'],
    step_table$byClass['Balanced Accuracy'],
    rf_table$byClass['Balanced Accuracy'],
    gbm_table$byClass['Balanced Accuracy'],
    xg_table$byClass['Balanced Accuracy']
  ))
}


colnames(monte_df)<-c(
  'LASSO ROC','Stepwise ROC', 'Random Forest ROC','GBM ROC','XGBoost ROC',
  'LASSO F1','Stepwise F1', 'Random Forest F1','GBM F1','XGBoost F1',
  'LASSO Precision','Stepwise Precision', 'Random Forest Precision','GBM Precision','XGBoost Precision',
  'LASSO Recall','Stepwise Recall', 'Random Forest Recall','GBM Recall','XGBoost Recall',
  'LASSO Balanced Accuracy','Stepwise Balanced Accuracy', 'Random Forest Balanced Accuracy', 'GBM Balanced Accuracy','XGBoost Balanced Accuracy'
)

```

## Summary Results

```{r}
# Mean Results
mean_results <- apply(monte_df, 2, mean)
print(mean_results)

```


```{r}
# Summary statistics
summary_stats <- data.frame(
  Metric = c("AUC", "F1", "Precision", "Recall", 'Balanced Accuracy'),
  LASSO = c(
    mean(monte_df$`LASSO ROC`), 
    mean(monte_df$`LASSO F1`), 
    mean(monte_df$`LASSO Precision`), 
    mean(monte_df$`LASSO Recall`),
    mean(monte_df$`LASSO Balanced Accuracy`)
  ),
  Stepwise = c(
    mean(monte_df$`Stepwise ROC`), 
    mean(monte_df$`Stepwise F1`), 
    mean(monte_df$`Stepwise Precision`), 
    mean(monte_df$`Stepwise Recall`),
    mean(monte_df$`Stepwise Balanced Accuracy`)
  ),
  RandomForest = c(
    mean(monte_df$`Random Forest ROC`), 
    mean(monte_df$`Random Forest F1`), 
    mean(monte_df$`Random Forest Precision`), 
    mean(monte_df$`Random Forest Recall`),
    mean(monte_df$`Random Forest Balanced Accuracy`)
  ),
  GBM = c(
    mean(monte_df$`GBM ROC`), 
    mean(monte_df$`GBM F1`), 
    mean(monte_df$`GBM Precision`), 
    mean(monte_df$`GBM Recall`),
    mean(monte_df$`GBM Balanced Accuracy`)
  ),
  XGBoost = c(
    mean(monte_df$`XGBoost ROC`), 
    mean(monte_df$`XGBoost F1`), 
    mean(monte_df$`XGBoost Precision`), 
    mean(monte_df$`XGBoost Recall`),
    mean(monte_df$`XGBoost Balanced Accuracy`)
  )
)

print(summary_stats)

```


