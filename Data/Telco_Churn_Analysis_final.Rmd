---
title: "Telco Customer Churn Analysis"
output: html_document
---

## Load required libraries

```{r}
library(dplyr)
library(ggplot2)
library(caret)
library(glmnet)
library(MASS)
library(corrplot)
library(pROC)
library(randomForest)
library(gbm)
library(xgboost)
library(cluster)
library(factoextra)
library(reshape2)
library(tidyr)
library(ggbeeswarm)
library(caret)
library(factoextra)
```

## Load and merge data

```{r}
status <- read.csv("Telco_customer_churn_status.csv")
demographics <- read.csv("Telco_customer_churn_demographics.csv")
location <- read.csv("Telco_customer_churn_location.csv")
services <- read.csv("Telco_customer_churn_services.csv")

data <- status %>%
  left_join(demographics, by = "Customer.ID") %>%
  left_join(location, by = "Customer.ID") %>%
  left_join(services, by = "Customer.ID")
```

## Select and clean variables

```{r}
selected_vars <- c("Churn.Label", "Gender", "Senior.Citizen", "Married", "Dependents", 
                   "Number.of.Dependents", "Tenure.in.Months", "Contract", 
                   "Phone.Service", "Multiple.Lines", "Internet.Service", 
                   "Online.Security", "Online.Backup", "Device.Protection.Plan", 
                   "Premium.Tech.Support", "Streaming.TV", "Streaming.Movies", 
                   "Paperless.Billing", "Payment.Method", "Monthly.Charge", 
                   "Total.Charges", "CLTV", "Satisfaction.Score")

data_model <- data %>%
  dplyr::select(all_of(selected_vars)) %>%
  na.omit() %>%
  mutate(Churn = as.factor(Churn.Label)) %>%
  dplyr::select(-Churn.Label)

```

## Exploratory Data Analysis

```{r}
churn_counts <- table(data_model$Churn)
churn_counts_df <- as.data.frame(churn_counts)
colnames(churn_counts_df) <- c("Churn", "count")
churn_counts_df$percentage <- (churn_counts_df$count / sum(churn_counts_df$count)) * 100
ggplot(churn_counts_df, aes(x = Churn, y = count)) + 
  geom_bar(stat = "identity", fill = "#FF6666") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), 
            vjust = -0.5, size = 5) + 
  labs(title = "Churn Distribution")

ggplot(data_model, aes(x = Churn, y = Monthly.Charge, fill = Churn)) +
  geom_boxplot() + labs(title = "Monthly Charge by Churn")

ggplot(data_model, aes(x = Contract, fill = Churn)) +
  geom_bar(position = "fill") + scale_y_continuous(labels = scales::percent) +
  labs(title = "Churn Rate by Contract Type")

num_vars <- data_model[, sapply(data_model, is.numeric)]
corrplot(cor(num_vars), method = "color", type = "upper", tl.cex = 0.8)
```

```{r}
total_lost<-sum(data['Churn.Value']*data['CLTV'])
cat("Lifetime value of churned customers: $",toString(round(total_lost,2)))
average_monthly_spend<-data['CLTV']/data['Tenure.in.Months']
average_monthly_churn<-sum(average_monthly_spend*data['Churn.Value'])
print("Monthly from a lost customer = Lifetime value / Tenure in Months")
cat(" Profit lost per month from churned customers: $",toString(round(average_monthly_churn,2)))
```

```{r}


#If we want to limit this, we can remove columns from here
selected_columns<-data[,c("Satisfaction.Score",
"Churn.Value",
"CLTV",
"Churn.Category",
"Churn.Reason",
"Gender",
"Age",
"Under.30",
"Senior.Citizen",
"Married",
"Dependents",
"Number.of.Dependents",
"Latitude",
"Longitude",
"Referred.a.Friend",
"Number.of.Referrals",
"Tenure.in.Months",
"Offer",
"Phone.Service",
"Avg.Monthly.Long.Distance.Charges",
"Multiple.Lines",
"Internet.Service",
"Internet.Type",
"Avg.Monthly.GB.Download",
"Online.Security",
"Online.Backup",
"Device.Protection.Plan",
"Premium.Tech.Support",
"Streaming.TV",
"Streaming.Movies",
"Streaming.Music",
"Unlimited.Data",
"Contract",
"Paperless.Billing",
"Payment.Method",
"Monthly.Charge",
"Total.Charges",
"Total.Refunds",
"Total.Extra.Data.Charges",
"Total.Long.Distance.Charges",
"Total.Revenue"
)]
one_hot_matrix<-model.matrix(~.-1,selected_columns)

melted_cormat <- melt(round(cor(one_hot_matrix),2), na.rm = TRUE)
ggplot(data = melted_cormat, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "#F8766D", high = "#00BFC4", mid = "white", 
   name="Correlation (rounded)") +
 theme(axis.text.x = element_text(angle = 90),axis.text.y = element_text())+
 coord_fixed()+
  geom_text(aes(Var2, Var1, label = value), color = "black", size = .5) +
  ggtitle("Correlation between variables")
ggsave('Project_all_big_cor.jpg', scale=3)


```

```{r}

  without_churn <-subset(one_hot_matrix, select = -c(2) ) 
  churn_value<-one_hot_matrix[,2]
  # Melt the data
  melted_viz <- melt(
    without_churn, 
    variable.name = "category",  # Name for the variable column
    value.name = "value"         # Name for the value column
  )
  
  # Filter for one-hot encoded variables (where value is 1)
  
  # Add back the churn column
  melted_viz$churn <- churn_value[rep(seq_len(nrow(data)), ncol(without_churn))]
  
  # Select and reorder columns
  
```

```{r}
ggplot(melted_viz, aes(x=value,group=as.factor(churn),fill=as.factor(churn))) + 
    geom_histogram(adjust=1.5)  + 
    facet_wrap(~Var2,scales = "free") +
  ggtitle("Density of Values by churn status (1 = Churned)")+
labs(fill='Churn (1 = churned)') 
ggsave('Churn_variable_histogram.jpg', scale=3)

```

## Train/test split and preprocessing

```{r}
set.seed(42)
split <- createDataPartition(data_model$Churn, p = 0.8, list = FALSE)
train <- data_model[split, ]
test  <- data_model[-split, ]

train <- train %>% mutate(across(where(is.character), as.factor))
test  <- test %>% mutate(across(where(is.character), as.factor))

valid_factors <- sapply(train, function(x) {
  if (is.factor(x)) nlevels(x) > 1 else TRUE
})
train <- train[, valid_factors]
test <- test[, names(train)]

nzv <- nearZeroVar(train, saveMetrics = TRUE)
train <- train[, !nzv$zeroVar]
test <- test[, names(train)]
#model.matrix(~.-1,selected_columns[,c(3,4,5,6,7,8,9,10,11,12,14,15,16)])
```

## Stepwise Logistic Regression

```{r}
full_model <- glm(Churn ~ ., data = train, family = "binomial")
step_model <- stepAIC(full_model, direction = "both", trace = FALSE)

step_pred_prob <- predict(step_model, newdata = test, type = "response")
step_pred <- ifelse(step_pred_prob > 0.5, "Yes", "No")
step_formula=step_model$formula
confusionMatrix(as.factor(step_pred), test$Churn)
step_roc <- roc(test$Churn, step_pred_prob)
plot(step_roc, main = "Stepwise Logistic ROC")
auc(step_roc)



```

## LASSO Logistic Regression

```{r}
x_train <- model.matrix(Churn ~ ., data = train)[, -1]
y_train <- ifelse(train$Churn == "Yes", 1, 0)
x_test <- model.matrix(Churn ~ ., data = test)[, -1]
y_test <- ifelse(test$Churn == "Yes", 1, 0)

cv_lasso <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1)
lasso_model <- glmnet(x_train, y_train, family = "binomial", lambda = cv_lasso$lambda.min)
lasso_pred_prob <- predict(lasso_model, newx = x_test, type = "response")
lasso_pred <- ifelse(lasso_pred_prob > 0.5, 1, 0)
confusionMatrix(as.factor(lasso_pred), as.factor(y_test))
lasso_roc <- roc(y_test, as.vector(lasso_pred_prob))
plot(lasso_roc, main = "LASSO Logistic ROC")
auc(lasso_roc)
```

## Model comparison table

```{r}
results <- data.frame(
  Model = c("Stepwise Logistic", "LASSO Logistic"),
  Accuracy = c(
    sum(step_pred == test$Churn) / length(step_pred),
    sum(lasso_pred == y_test) / length(lasso_pred)
  ),
  AUC = c(auc(step_roc), auc(lasso_roc))
)
print(results)
```

## Extended models: RF, GBM, XGBoost, Clustering

```{r}
#rf_model <- randomForest(Churn ~ ., data = train, ntree = 100)
#rf_pred <- predict(rf_model, newdata = test)
#confusionMatrix(rf_pred, test$Churn)
#varImpPlot(rf_model)
rf_grid <- expand.grid(
  mtry = c(
    sqrt(ncol(train) - 1),          # Square root of the number of features
    log2(ncol(train) - 1),          # Log2 of the number of features
    (ncol(train) - 1) / 3,          # One-third of the number of features
    floor(ncol(train) / 2),         # Half of the number of features
    floor(ncol(train) * 0.8),      # 80% of the number of features
    ncol(train) - 1                # All features except the target variable
  )
)

# Train using cross-validation
rf_control <- trainControl(method = "cv", number = 5)  # 5-fold 

rf_tuned <- train(
  Churn ~ ., 
  data = train, 
  method = "rf", 
  trControl = rf_control, 
  tuneGrid = rf_grid,
  ntree = 100
)

# Best parameters
print(rf_tuned$bestTune)

# Make predictions
rf_pred <- predict(rf_tuned, newdata = test)

# Evaluate performance
confusionMatrix(rf_pred, test$Churn)

# Feature importance
print(varImp(rf_tuned))
print(varImpPlot(rf_tuned$finalModel))

# gbm
# Prepare the data
train_gbm <- train %>%
  mutate(Churn = factor(ifelse(Churn == "Yes", 1, 0), levels = c(0, 1), labels = c("No", "Yes")))

test_gbm <- test %>%
  mutate(Churn = factor(ifelse(Churn == "Yes", 1, 0), levels = c(0, 1), labels = c("No", "Yes")))

# Set up training control
ctrl <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

# Set up grid for hyperparameter tuning
gbm_grid <- expand.grid(
  n.trees = c(100, 200, 300),
  interaction.depth = c(3, 4, 5),
  shrinkage = c(0.01, 0.1),
  n.minobsinnode = 10
)

# Train GBM model using caret
gbm_model <- train(
  Churn ~ ., 
  data = train_gbm, 
  method = "gbm",
  trControl = ctrl,
  tuneGrid = gbm_grid,
  metric = "ROC",
  verbose = FALSE
)

# Print best model
print(gbm_model$bestTune)

# Predict on test set
gbm_pred <- predict(gbm_model, newdata = test_gbm)
gbm_pred_prob <- predict(gbm_model, newdata = test_gbm, type = "prob")

# Evaluate the model
confusionMatrix(gbm_pred, test_gbm$Churn)

# XGBoost
# Convert training and testing data into xgb.DMatrix format
xgb_train <- xgb.DMatrix(data = x_train, label = y_train)
xgb_test <- xgb.DMatrix(data = x_test, label = y_test)

# Define hyperparameter grid
xgb_grid <- expand.grid(
  nrounds = c(50, 100, 150),  # Number of boosting rounds
  eta = c(0.01, 0.1, 0.3),    # Learning rate
  max_depth = c(3, 6, 9),     # Tree depth
  gamma = c(0, 1, 5),          # Minimum loss reduction
  colsample_bytree = c(0.5, 0.7),
  min_child_weight = c(1, 5),
  subsample = c(0.6, 0.8, 1)  # Required parameter
)

# Define training control
train_control <- trainControl(
  method = "cv",         # Cross-validation
  number = 5,            # 5-fold CV
  verboseIter = FALSE     # Show training progress
)

# Train the model using caret
xgb_model <- train(
  x = as.matrix(x_train), 
  y = as.factor(y_train), 
  method = "xgbTree",
  trControl = train_control,
  tuneGrid = xgb_grid
)

print(xgb_model$bestTune)

# Predict on test set
xgb_pred_prob <- predict(xgb_model, newdata = as.matrix(x_test), type = "prob")[,2]
xgb_pred <- ifelse(xgb_pred_prob > 0.5, 1, 0)

# Evaluate with confusion matrix
confusionMatrix(as.factor(xgb_pred), as.factor(y_test))



#numeric_scaled <- scale(data_model[, sapply(data_model, is.numeric)])
numeric_scaled<-scale(one_hot_matrix)
k_values <- 2:10  # Change this range as needed
wss_values <- numeric(length(k_values))  # Store WSS values

# Perform grid search over different k values
for (i in seq_along(k_values)) {
  k <- k_values[i]
  km <- kmeans(numeric_scaled, centers = k, nstart = 25)
  wss_values[i] <- km$tot.withinss  # Store WSS
}

# Plot the WSS values to determine the best k
plot(k_values, wss_values, type = "b", pch = 19, frame = FALSE,
     xlab = "Number of clusters (k)", ylab = "Within-Cluster Sum of Squares (WSS)")

# Choose the best k based on the elbow method (or silhouette analysis)
optimal_k <- k_values[which.min(diff(diff(wss_values))) + 1]
cat("Best k (optimal number of clusters):", optimal_k, "\n")
# Final clustering using optimal_k
final_km <- kmeans(numeric_scaled, centers = optimal_k, nstart = 25)

# Visualize the final clustering
fviz_cluster(final_km, data = numeric_scaled)
```

```{r}




unscaled_centers <- (km$centers * sapply(data.frame(one_hot_matrix), sd,2)) + colMeans(one_hot_matrix) 
#ggplot(melt(unscaled_centers), aes(x=as.factor(Var1), y=value)) + 
#  geom_point(stat = "identity") +
#facet_wrap(~Var2,scale="free") +
#  ggtitle("Average score by variable, by cluster ")
#unscaled_centers
#ggsave('cluster_avgs.jpg', scale=3)
one_hot_matrix_clustered<-one_hot_matrix
one_hot_matrix_clustered<-cbind(one_hot_matrix_clustered,km$cluster)


#ggplot(pivot_longer(data.frame(one_hot_matrix_clustered),c(1:73)), aes(x=value,group=as.factor(V74),fill=as.factor(V74))) + 
 #   geom_histogram(adjust=1.5,alpha=.5)  + 
  ##  facet_wrap(~name,scales = "free")+
  #labs(fill='Cluster')  +
 # ggtitle("Histogram by variable, by cluster (original values)")
#ggsave('cluster_histograms.jpg', scale=3)



ggplot(pivot_longer(data.frame(one_hot_matrix_clustered),c(1:73)), aes(y=value,x=as.factor(V74))) + 
   geom_jitter(size=0.001, alpha=0.2) +
  aes(color=as.factor(V74)) +
  facet_wrap(~name,scales = "free")+
  labs(color='Cluster') +
  ggtitle("Variables values by cluster (scatter plot)")

  ggsave('cluster_jitter.jpg', scale=3)

```

```{r}

ggplot(pivot_longer(data.frame(one_hot_matrix_clustered),c(1:73)), aes(y=value,x=as.factor(V74))) + 
   geom_boxplot(weight=0.1,fill=NA) +
  aes(color=as.factor(V74)) +
  facet_wrap(~name,scales = "free")+
  labs(color='Cluster')  +
  ggtitle("Variables values by cluster (box plot)")
  ggsave('cluster_boxplot.jpg', scale=3)

```

## Model Comparison

```{r}
# Model comparison for Stepwise, LASSO, RF, GBM, XGBoost

# Convert any needed factors for fairness
rf_auc <- roc(test$Churn, as.numeric(rf_pred == "Yes"))
gbm_auc <- roc(test_gbm$Churn, gbm_pred_prob)
xgb_auc <- roc(y_test, xgb_pred_prob)

results_all <- data.frame(
  Model = c("Stepwise Logistic", "LASSO Logistic", "Random Forest", "GBM", "XGBoost"),
  Accuracy = c(
    mean(step_pred == test$Churn),
    mean(lasso_pred == y_test),
    mean(rf_pred == test$Churn),
    mean(gbm_pred == test_gbm$Churn),
    mean(xgb_pred == y_test)
  ),
  AUC = c(
    auc(step_roc),
    auc(lasso_roc),
    auc(rf_auc),
    auc(gbm_auc),
    auc(xgb_auc)
  )
)

print(results_all)

```

```{r}
#Variable importance??
```

# Cross-Validation

```{r}

set.seed(123)
B=100
monte_df<-data.frame()


for (b in 1:B){
  if (b %% 20 == 0) {
    print(paste("Loop", b, "of", B))
  }
  
  split <- createDataPartition(data_model$Churn, p = 0.8, list = FALSE)
  train <- data_model[split, ]
  test  <- data_model[-split, ]
  
  # Data Pre-processing
  train <- train %>% mutate(across(where(is.character), as.factor))
  test  <- test %>% mutate(across(where(is.character), as.factor))

  valid_factors <- sapply(train, function(x) {
    if (is.factor(x)) nlevels(x) > 1 else TRUE
  })
  train <- train[, valid_factors]
  test <- test[, names(train)]

  nzv <- nearZeroVar(train, saveMetrics = TRUE)
  train <- train[, !nzv$zeroVar]
  test <- test[, names(train)]

  
  #Prepare Data 
  x_train <- model.matrix(Churn ~ ., data = train)[, -1]
  y_train <- ifelse(train$Churn == "Yes", 1, 0)
  x_test <- model.matrix(Churn ~ ., data = test)[, -1]
  y_test <- ifelse(test$Churn == "Yes", 1, 0)


  ### Model 1: Stepwise
  #The below code re-fits stepwise with every regression run. That is how we used to do it
  #full_model <- glm(Churn ~ ., data = train, family = "binomial")
  #step_model <- stepAIC(full_model, direction = "both", trace = FALSE)
  
  #This code uses the same selected variables every time.
  step_model <- glm(step_formula, data = train, family = "binomial")
  step_pred_prob <- predict(step_model, newdata = test, type = "response")
  step_pred <- ifelse(step_pred_prob > 0.5, "Yes", "No")

  ### Model 2: LASSO
  lasso_model <- glmnet(x_train, y_train, family = "binomial", lambda = cv_lasso$lambda.min)
  lasso_pred_prob <- predict(lasso_model, newx = x_test, type = "response")
  lasso_pred <- ifelse(lasso_pred_prob > 0.5, 1, 0)

  ### Model 3: Random Forest
  rf_model <- randomForest(Churn ~ ., data = train, ntree = 100) 
  rf_pred_prob <- predict(rf_model, newdata = test, type = "prob")[, "Yes"] 
  rf_pred <- predict(rf_model, newdata = test) 
  
  ### Model 4: GBM
  train_gbm <- train %>%
    mutate(Churn = ifelse(Churn == "Yes", 1, 0))
  test_gbm <- test %>%
    mutate(Churn = ifelse(Churn == "Yes", 1, 0))

  gbm_model <- gbm(Churn ~ ., data = train_gbm, distribution = "bernoulli",
                 n.trees = best_iter, shrinkage = 0.01, interaction.depth = 3)

  gbm_pred_prob <- predict(gbm_model, newdata = test_gbm, n.trees = best_iter, type = "response")
  gbm_pred <- ifelse(gbm_pred_prob > 0.5, 1, 0)

  
  ### Model 5: XGBoost
  xgb_train <- xgb.DMatrix(data = x_train, label = y_train)
  xgb_test <- xgb.DMatrix(data = x_test, label = y_test)
  xgb_model <- xgboost(data = xgb_train, objective = "binary:logistic", nrounds = 100, verbose = 0)
  xgb_pred_prob <- predict(xgb_model, newdata = xgb_test)
  xgb_pred <- ifelse(xgb_pred_prob > 0.5, 1, 0)

  ### Metrics
  step_table<-confusionMatrix(as.factor(step_pred), test$Churn)
  lasso_table<-confusionMatrix(as.factor(lasso_pred), as.factor(y_test))
  rf_table<-confusionMatrix(as.factor(rf_pred), test$Churn)
  gbm_table<-confusionMatrix(as.factor(gbm_pred), as.factor(test_gbm$Churn))
  xg_table<-confusionMatrix(as.factor(xgb_pred), as.factor(y_test))

  lasso_roc <- roc(y_test, as.vector(lasso_pred_prob))
  step_roc <- roc(y_test, as.vector(step_pred_prob))
  rf_roc <- roc(y_test, as.vector(rf_pred_prob))  
  gbm_roc <- roc(y_test, as.vector(gbm_pred_prob))
  xgb_roc <- roc(y_test, as.vector(xgb_pred_prob))


  monte_df<-rbind(monte_df,c(
    lasso_roc$auc,
    step_roc$auc,
    rf_roc$auc,  
    gbm_roc$auc,
    xgb_roc$auc,
    
    lasso_table$byClass['F1'],
    step_table$byClass['F1'],
    gbm_table$byClass['F1'],
    rf_table$byClass['F1'],
    xg_table$byClass['F1'],
    
    lasso_table$byClass['Precision'],
    step_table$byClass['Precision'],
    gbm_table$byClass['Precision'],
    rf_table$byClass['Precision'],
    xg_table$byClass['Precision'],
    
    lasso_table$byClass['Recall'],
    step_table$byClass['Recall'],
    gbm_table$byClass['Recall'],
    rf_table$byClass['Recall'],
    xg_table$byClass['Recall'],
    
    lasso_table$byClass['Balanced Accuracy'],
    step_table$byClass['Balanced Accuracy'],
    rf_table$byClass['Balanced Accuracy'],
    gbm_table$byClass['Balanced Accuracy'],
    xg_table$byClass['Balanced Accuracy']
  ))
}


colnames(monte_df)<-c(
  'LASSO ROC','Stepwise ROC', 'Random Forest ROC','GBM ROC','XGBoost ROC',
  'LASSO F1','Stepwise F1', 'Random Forest F1','GBM F1','XGBoost F1',
  'LASSO Precision','Stepwise Precision', 'Random Forest Precision','GBM Precision','XGBoost Precision',
  'LASSO Recall','Stepwise Recall', 'Random Forest Recall','GBM Recall','XGBoost Recall',
  'LASSO Balanced Accuracy','Stepwise Balanced Accuracy', 'Random Forest Balanced Accuracy', 'GBM Balanced Accuracy','XGBoost Balanced Accuracy'
)

```

## Summary Results

```{r}
# Mean Results
mean_results <- apply(monte_df, 2, mean)
print(mean_results)

```

```{r}
# Summary statistics
summary_stats <- data.frame(
  Metric = c("AUC", "F1", "Precision", "Recall", 'Balanced Accuracy'),
  LASSO = c(
    mean(monte_df$`LASSO ROC`), 
    mean(monte_df$`LASSO F1`), 
    mean(monte_df$`LASSO Precision`), 
    mean(monte_df$`LASSO Recall`),
    mean(monte_df$`LASSO Balanced Accuracy`)
  ),
  Stepwise = c(
    mean(monte_df$`Stepwise ROC`), 
    mean(monte_df$`Stepwise F1`), 
    mean(monte_df$`Stepwise Precision`), 
    mean(monte_df$`Stepwise Recall`),
    mean(monte_df$`Stepwise Balanced Accuracy`)
  ),
  RandomForest = c(
    mean(monte_df$`Random Forest ROC`), 
    mean(monte_df$`Random Forest F1`), 
    mean(monte_df$`Random Forest Precision`), 
    mean(monte_df$`Random Forest Recall`),
    mean(monte_df$`Random Forest Balanced Accuracy`)
  ),
  GBM = c(
    mean(monte_df$`GBM ROC`), 
    mean(monte_df$`GBM F1`), 
    mean(monte_df$`GBM Precision`), 
    mean(monte_df$`GBM Recall`),
    mean(monte_df$`GBM Balanced Accuracy`)
  ),
  XGBoost = c(
    mean(monte_df$`XGBoost ROC`), 
    mean(monte_df$`XGBoost F1`), 
    mean(monte_df$`XGBoost Precision`), 
    mean(monte_df$`XGBoost Recall`),
    mean(monte_df$`XGBoost Balanced Accuracy`)
  )
)

print(summary_stats)

```

```{r}
roc_plot<-ggplot(data=melt(monte_df[,1:5]),
aes(x=value, group=variable, fill=variable)) +
    geom_density(adjust=1.5,alpha=.5) +
  ggtitle("ROC, across 100 Monte Carlo Runs") +
labs(fill='Model') 
F1_plot<-ggplot(data=melt(monte_df[,6:10]),
aes(x=value, group=variable, fill=variable)) +
    geom_density(adjust=1.5,alpha=.5) +
  ggtitle("F1, across 100 Monte Carlo Runs") +
labs(fill='Model') 
prec_plot<-ggplot(data=melt(monte_df[,11:15]),
aes(x=value, group=variable, fill=variable)) +
    geom_density(adjust=1.5,alpha=.5) +
  ggtitle("Precision, across 100 Monte Carlo Runs") +
labs(fill='Model') 
recall_plot<-ggplot(data=melt(monte_df[,16:20]),
aes(x=value, group=variable, fill=variable)) +
    geom_density(adjust=1.5,alpha=.5) +
ggtitle("Recall, across 100 Monte Carlo Runs") +
labs(fill='Model') 
acc_plot<-ggplot(data=melt(monte_df[,21:25]),
aes(x=value, group=variable, fill=variable)) +
    geom_density(adjust=1.5,alpha=.5) +
ggtitle("Blanced Accuracy, across 100 Monte Carlo Runs") +
labs(fill='Model') 



library(gridExtra)
monte_density<-arrangeGrob(roc_plot, F1_plot,prec_plot, recall_plot,acc_plot,nrow = 3)
ggsave("Monte_density.jpg",monte_density,scale=2)

```

## Statistical Tests

```{r}
 get_p_value <- function(input){
      as.numeric(unlist(input)[2])

 }
 shapiro_tests<-apply(monte_df,2,shapiro.test)
#tests for normality
#normality<-c(shapiro.test(monte_df[,1])$p.value,
normality<-data.frame(lapply(shapiro_tests,get_p_value))
#Random Forest Recall and ROC are the only ones that fail normalcy by a lot, GBM precision, LASSO Recall, and XGBoost ROC are on the borderline, but we can still probably use them. Random Forest is so good on the recall and so bad on the ROC it's probably relative, and we have a good run
#Random forest 



normality[normality < .05]
#All are close to normal, except Random Forest. Could replace with Wilcoxon-Signed Rank

T1_roc=t.test(monte_df[,2],monte_df[,1],paired=T)
T2_roc=t.test(monte_df[,2],monte_df[,3],paired=T)
T3_roc=t.test(monte_df[,2],monte_df[,4],paired=T)
T4_roc=t.test(monte_df[,2],monte_df[,5],paired=T)
t_scores_roc<-c(T1_roc$p.value,T2_roc$p.value,T3_roc$p.value,T4_roc$p.value)


T1_balance=t.test(monte_df[,22],monte_df[,21],paired=T)
T2_balance=t.test(monte_df[,22],monte_df[,23],paired=T)
T3_balance=t.test(monte_df[,22],monte_df[,24],paired=T)
T4_balance=t.test(monte_df[,22],monte_df[,25],paired=T)
T1_precision=t.test(monte_df[,12],monte_df[,11],paired=T)
T2_precision=t.test(monte_df[,12],monte_df[,13],paired=T)
T3_precision=t.test(monte_df[,12],monte_df[,14],paired=T)
T4_precision=t.test(monte_df[,12],monte_df[,15],paired=T)
t_scores_balance<-c(T1_balance$p.value,T2_balance$p.value,T3_balance$p.value,T4_balance$p.value)
t_scores_precision<-c(T1_precision$p.value,T2_precision$p.value,T3_precision$p.value,T4_precision$p.value)

T1_F1=t.test(monte_df[,9],monte_df[,6],paired=T)
T2_F1=t.test(monte_df[,9],monte_df[,7],paired=T)
T3_F1=t.test(monte_df[,9],monte_df[,8],paired=T)
T4_F1=t.test(monte_df[,9],monte_df[,10],paired=T)
t_scores_F1<-c(T1_F1$p.value,T2_F1$p.value,T3_F1$p.value,T4_F1$p.value)

T1_recall=t.test(monte_df[,18],monte_df[,16],paired=T)
T2_recall=t.test(monte_df[,18],monte_df[,17],paired=T)
T3_recall=t.test(monte_df[,18],monte_df[,19],paired=T)
T4_recall=t.test(monte_df[,18],monte_df[,20],paired=T)
t_scores_recall<-c(T1_recall$p.value,T2_recall$p.value,T3_recall$p.value,T4_recall$p.value)
#t_scores_step<-c(T1_roc$p.value,T2_roc$p.value,T3_roc$p.value,T4_roc$p.value)
comp_names_step<-c( "Stepwise = LASSO?","Stepwise = Random Forest?","Stepwise = Naive GBM?"," Stepwise = XGBoost?")
t_table_stepwise<-data.frame(
  Hypothesis=comp_names_step,
  p_value_roc=t_scores_roc,
  p_value_precision=t_scores_precision,
  p_value_balanced_accuracy=t_scores_balance
)
comp_names_gbm<-c( "Naive GBM = LASSO?","Naive GBM = Stepwise?","Naive GBM = Random Forest?"," Naive GBM = XGBoost?")
comp_names_rf<-c( "Random Forest = LASSO?","Random Forest = Stepwise?","Random Forest = Naive GBM?"," Random Forest = XGBoost?")

t_table_GBM<-data.frame(
  Hypothesis=comp_names_gbm,
  p_value_f1=t_scores_F1
)
t_table_rf<-data.frame(
  Hypothesis=comp_names_rf,
  p_value_recall=t_scores_recall
)
t_table_stepwise
t_table_GBM
print(t_table_rf)
#Best:

#Recall: Random Forest
#Precision: Stepwise
#F1: GBM
#ROC: Stepwise
#Balanced Accuracy: Stepwise


#Looking at our selected variables:
#Referrals did not help our model
#Charge/total charge/LTV did matter
#Paperless billing, online backup, online securirty did matter
#Geography did not matter
#Contract terms did matter
```
